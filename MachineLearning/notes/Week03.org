* Week 03
[2016-08-05 Fri 16:15]
Welcome to week 3! This week, we’ll be covering logistic regression. Logistic regression is a method for classifying data into discrete outcomes. For example, we might use logistic regression to classify an email as spam or not spam. In this module, we introduce the notion of classification, the cost function for logistic regression, and the application of logistic regression to multi-class classification.

We are also covering regularization. Machine learning models need to generalize well to new examples that the model has not seen in practice. We’ll introduce regularization, which helps prevent models from overfitting the training data.

As always, if you get stuck on the quiz and programming assignment, you should post on the Discussions to ask for help. (And if you finish early, I hope you'll go there to help your fellow classmates as well.)
** Classification and Representation
[2016-08-05 Fri 16:15]
*** Classification
[2016-08-05 Fri 16:15]
What is classification -- spam/not spam, fraud detection, tumor malignancy. y can take on two values, 0 or 1. Can extend to multi-class classification problems, where y can take on more than yes/no types of values. Can set thresholds -- if > 0.5 then set to 1, < 0.5 set to 0. Problem of extreme outliers for linear regression giving a worse hypothesis. Use logistic regression to ensure that the output is between 0 and 1.
*** Hypothesis Representation
[2016-08-05 Fri 16:16]
h_{0}(x) = g(\theta^{T}x), where g(z) = \frac{1}{1 + e^{-z}}. g(z) is the sigmoid or logistic function. Plot of this has asymptotes of 1 as \limit{Z} approaches \infinity and 0 as \limit{z} approaches -\infinity. This turns h_{0}(x) into the probability that y = 1 on input x. Probability of y = 0 becomes 1 - P(y = 1|x, \theta). 
*** Decision Boundary
[2016-08-05 Fri 16:16]
For logistic regression, you have y = 1 whenever z >= 0, so this means that \theta^{T}x >=0 is the positive case. Conversely, y = 0 whenever \theta^{T}x < 0. Decision boundary - line where you cross from negative hypothesis to positive. Non-linear decision boundaries - add more complex equations (square terms).
** Logistic Regression Model
[2016-08-05 Fri 16:46]
*** Cost Function
[2016-08-05 Fri 16:47]
If we just use the logistic function, then looking at graph for J(\theta) is non-convex, has many local optima and not guaranteed to converge to the global minimum. Change the cost function to be -log(h_{\theta}(x)) if y = 1, and -log(1 - h_{\theta}(x)) if y = 0. Looking at graphs, approaches \inf asymptotically for the y = 0 and y = 1 cases.
*** Simplified Cost Function and Gradient Descent
[2016-08-05 Fri 16:47]
Cost(h_{\theta}(x), y) = -y log(h_{\theta}(x)) - (1 - y) log(1 - h_{\theta}(x)), more efficient way to represent this. Can see how this works by setting y to 1 or 0 and see the appropriate term drop out. How to plug this into gradient descent -- cosmetically looks the same, but the h_{\theta}(x) has changed from \theta^{T} * x to \frac{1}{1 + e^{\theta^{T} * x}}.
*** Advanced Optimization
[2016-08-05 Fri 16:47]
Given J(\theta) and \frac{\partial}{\partial \theta_{j}}, can use this in other algorithms besides gradient descent. Conjugate gradient, [[https://en.wikipedia.org/wiki/Broyden%25E2%2580%2593Fletcher%25E2%2580%2593Goldfarb%25E2%2580%2593Shanno_algorithm][BFGS]], [[https://en.wikipedia.org/wiki/Limited-memory_BFGS][L-BFGS]].
** Multiclass Classification
[2016-08-12 Fri 14:48]
*** Multiclass classification: one-vs.-all
[2016-08-12 Fri 14:48]
What is multiclass classification? E-mail tagging (work, family, etc.), weather (sunny, cloudy, rain, snow). Go through and turn everything into that category and "everything else". Then, after constructing your i classifiers run new input x through all and see which one is best fit.
** Regularization
[2016-08-12 Fri 14:54]
*** The problem of overfitting
[2016-08-12 Fri 15:19]
``underfit'' or ``high-bias''. ``overfit'' or ``high variance''. Overfitting -- fits the training set very well, but doesn't generalize to new examples well. Many features but not a lot of training data. Methods to deal with this include feature reduction, model selection algorithms. Alternatively, regularization is keeping all the features but reducing the magnitude of the parameters \theta.
*** Cost function
[2016-08-12 Fri 15:27]
Take cost function and modify it so that some of the parameters are small, which could make a 4th order polynomial into a second order one as two terms go close to zero. Add term to the end -- \lambda * \sum{i = 1}{m} \theta_{j}^{2}. If you make \lambda too large, then you drive all the terms to effectively 0, and you get a flat line (aka underfit).
*** Regularized Linear Regression
[2016-08-12 Fri 15:36]
Don't penalize \theta_{0}. Just add \frac{\lambda}{m} \theta_{j} to gradient descent formula. Can rearrange, to get \theta_{j}(1 - \alpha \frac{\lambda}{m}) as the first term for gradient descent. This is < 1 (not negative, dammit!).
*** Regularized Logistic Regression
[2016-08-12 Fri 15:37]
* Notes
[2016-08-15 Mon 11:12]
Cost function needs to be > 0, but want it small (and to decrease over time). Octave cheat sheet. 

