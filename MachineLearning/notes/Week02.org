* Week 02
[2016-08-01 Mon 11:22]
** Multivariate Linear Regression
[2016-08-03 Wed 11:08]
*** Multiple Features
[2016-08-01 Mon 12:12]
More general case - n number of features. x_{j}^{i} - the value of feature j in the ith training example. Now, hypothesis goes to h_{\theta}(x) = \theta_{0} + \theta_{1} * x_{1} + \theta_{2} * x_{2} + ... + \theta_{n} * x_{n}. Treat x and \theta as parameter vectors, with x_{0} = 1. Now we can have an x term to pair up with \theta_{0}. Can then simplify the equation to a matrix one, h_{0}(x) = \theta^{T} * x. Also known as multivariate linear regression.
*** Gradient Descent for Multiple Variables 
[2016-08-01 Mon 12:12]
*** Gradient Descent in Practice I - Feature Scaling
[2016-08-01 Mon 13:42]
Make sure features are on a similar scale. House example -- size goes from 0 - 2000 (sq. feet), number of bedrooms goes from 1 - 5. Convert to ratios (size / 2000, bedrooms / 5). In general, try to get between -1 and 1 (approximately). Generally, range between -3 to -1/3, and 1/3 to 3. Can also do mean normalization.
*** Gradient Descent in Practice II - Learning Rate
[2016-08-01 Mon 13:49]
``Debugging'', plus how to choose a rate \alpha.

For debugging/verifying that running correctly, look to plot J(\theta). Should see J(\theta) decrease after every iteration. Very hard to tell in advance how many iterations are needed, but plot can help with that. Can also stop if J(\theta) decreases by less than some specified amount.

If issues with convergence, generally better to pick a smaller \alpha. The graph for slow convergence -- graph B from the lecture. 0.001 -> 0.003 -> 0.01 -> 0.03 -> 0.1 ....
*** Feature and Polynomial Regression
[2016-08-01 Mon 13:58]
House example - move from frontage and depth of lot to area (frontage * depth).
** Computing Parameters Analytically
[2016-08-03 Wed 11:08]
*** Normal Equation
[2016-08-03 Wed 11:08]
Lets you solve for \theta in one step, rather than iterative method of gradient descent. Assume simple quadratic equation for J(\theta) - take first derivative, set equal to 0, then solve for \theta. For a more general cost function, take series of partial derivatives, set to 0, solve for each individual \theta. For this method, take all our features (including x_{0} = 1), and create an m x n+1 matrix. Y becomes an m-dimensional vector. Then, we get to

\theta = (X^{T} * X)^{-1} * X^{T}*y

In Octave

`pinv(X'*X)*X'*y`

If you're using the normal equation, then you don't need to worry about feature scaling.

Gradient descent - disadvantages: need to choose \alpha, need many iterations. Advantages: works well even if n is very large.
Normal equation - disadvantages: calculating (X^{T} * X)^{-1} is roughly an order n^{3} operation. Slow if n is very large. Advantages: don't need to pick \alpha, don't need to iterate. If n is ~10,000 then you need to start thinking of switching from normal equation to gradient descent. 
*** Normal Equation Noninvertibility
[2016-08-03 Wed 11:08]
What if you have X^{T}*X is a case of a singular/degenerate matrix that is non-invertable? Rare, but at least in Octave pinv will handle this for you. See this in cases where you have either redundant (linearly dependent) features (e.g. size in both m^{2} and ft^{2}), or if you  have more features than you have examples (m < n), in which case you can either delete features or regularize.
