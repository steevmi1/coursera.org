* Week 02
[2016-08-01 Mon 11:22]
Welcome to week 2! I hope everyone has been enjoying the course and learning a lot! This week we’re covering linear regression with multiple variables. we’ll show how linear regression can be extended to accommodate multiple input features. We also discuss best practices for implementing linear regression.

We’re also going to go over how to use Octave. You’ll work on programming assignments designed to help you understand how to implement the learning algorithms in practice. To complete the programming assignments, you will need to use Octave or MATLAB.

As always, if you get stuck on the quiz and programming assignment, you should post on the Discussions to ask for help. (And if you finish early, I hope you'll go there to help your fellow classmates as well.)
** Multivariate Linear Regression
[2016-08-03 Wed 11:08]
*** Multiple Features
[2016-08-01 Mon 12:12]
More general case - n number of features. x_{j}^{i} - the value of feature j in the ith training example. Now, hypothesis goes to h_{\theta}(x) = \theta_{0} + \theta_{1} * x_{1} + \theta_{2} * x_{2} + ... + \theta_{n} * x_{n}. Treat x and \theta as parameter vectors, with x_{0} = 1. Now we can have an x term to pair up with \theta_{0}. Can then simplify the equation to a matrix one, h_{0}(x) = \theta^{T} * x. Also known as multivariate linear regression.
*** Gradient Descent for Multiple Variables 
[2016-08-01 Mon 12:12]
*** Gradient Descent in Practice I - Feature Scaling
[2016-08-01 Mon 13:42]
Make sure features are on a similar scale. House example -- size goes from 0 - 2000 (sq. feet), number of bedrooms goes from 1 - 5. Convert to ratios (size / 2000, bedrooms / 5). In general, try to get between -1 and 1 (approximately). Generally, range between -3 to -1/3, and 1/3 to 3. Can also do mean normalization.
*** Gradient Descent in Practice II - Learning Rate
[2016-08-01 Mon 13:49]
``Debugging'', plus how to choose a rate \alpha.

For debugging/verifying that running correctly, look to plot J(\theta). Should see J(\theta) decrease after every iteration. Very hard to tell in advance how many iterations are needed, but plot can help with that. Can also stop if J(\theta) decreases by less than some specified amount.

If issues with convergence, generally better to pick a smaller \alpha. The graph for slow convergence -- graph B from the lecture. 0.001 -> 0.003 -> 0.01 -> 0.03 -> 0.1 ....
*** Feature and Polynomial Regression
[2016-08-01 Mon 13:58]
House example - move from frontage and depth of lot to area (frontage * depth).
** Computing Parameters Analytically
[2016-08-03 Wed 11:08]
*** Normal Equation
[2016-08-03 Wed 11:08]
Lets you solve for \theta in one step, rather than iterative method of gradient descent. Assume simple quadratic equation for J(\theta) - take first derivative, set equal to 0, then solve for \theta. For a more general cost function, take series of partial derivatives, set to 0, solve for each individual \theta. For this method, take all our features (including x_{0} = 1), and create an m x n+1 matrix. Y becomes an m-dimensional vector. Then, we get to

\theta = (X^{T} * X)^{-1} * X^{T}*y

In Octave

`pinv(X'*X)*X'*y`

If you're using the normal equation, then you don't need to worry about feature scaling.

Gradient descent - disadvantages: need to choose \alpha, need many iterations. Advantages: works well even if n is very large.
Normal equation - disadvantages: calculating (X^{T} * X)^{-1} is roughly an order n^{3} operation. Slow if n is very large. Advantages: don't need to pick \alpha, don't need to iterate. If n is ~10,000 then you need to start thinking of switching from normal equation to gradient descent. 
*** Normal Equation Noninvertibility
[2016-08-03 Wed 11:08]
What if you have X^{T}*X is a case of a singular/degenerate matrix that is non-invertable? Rare, but at least in Octave pinv will handle this for you. See this in cases where you have either redundant (linearly dependent) features (e.g. size in both m^{2} and ft^{2}), or if you  have more features than you have examples (m < n), in which case you can either delete features or regularize.
** Week 2 quiz
[2016-08-03 Wed 11:47]
1 - Mean is 81. 
2 - α=0.3 is an effective choice of learning rate.
3 - X is 28×5, y is 28×1, θ is 5×1
4 - Gradient descent, since (XTX)−1 will be very slow to compute in the normal equation.
5 - It speeds up gradient descent by making it require fewer iterations to get to a good solution.

Got the first question wrong, rest right (80%). Calculated SD for denominator, suspect should have used max(x) - min(x), which would have given 0.32, may retake?
** Octave tutorials
[2016-08-03 Wed 11:56]
Setting prompt -- PS1(). Semicolon suppressing output at end of a line. % for quote character.

Matrix - A = [1 2; 3 4; 5 6] generates a 3 x 2 matrix. Row vector - V = [1 2 3], column vector - V = [1; 2; 3]. Use colon to denote ranges, can make second arguement step and third stop, or start and stop. ones() to generate a matrix of all ones, zeros() to generate one of all zeros, rand() to generate a random matrix, randn() to use gaussian distribution, eye() to generate identity matrix. who() to show what things are loaded in the environment. whos() to show more detail (type info, etc.). Need to use parentheses to refer to matrix elements, not square brackets.

'.' to denote element-wise operations -- A .* B multiplies A(1 1) with B(1 1), A(1,2) with B(1,2), ....

''' to identify matrix transpose -- A'

for i=1:10,
   v(i) = 2^i;
end

if i == 6,
  break;
end

Define functions -- create a file named "function.m", then Octave will use that. Octave search path - how to set this in the shell?

Arrays start at 1, not 0.

