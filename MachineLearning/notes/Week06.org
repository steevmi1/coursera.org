* Week 06
[2016-08-30 Tue 12:02]
In Week 6, you will be learning about systematically improving your learning algorithm. The videos for this week will teach you how to tell when a learning algorithm is doing poorly, and describe the 'best practices' for how to 'debug' your learning algorithm and go about improving its performance.

We will also be covering machine learning system design. To optimize a machine learning algorithm, youâ€™ll need to first understand where the biggest improvements can be made. In these lessons, we discuss how to understand the performance of a machine learning system with multiple parts, and also how to deal with skewed data.

When you're applying machine learning to real problems, a solid grasp of this week's content will easily save you a large amount of work.
** Evaluating a Learning Algorithm
[2016-08-30 Tue 12:02]
*** Deciding What to Try Next
[2016-08-30 Tue 12:03]
Difference between knowing algos and understanding when and how to apply. How to improve if predictions are off? More examples, but sometimes that doesn't actually help. Smaller or larger sets of features, polynomial features, change value for \lambda. ``Gut feeling'' not always the best way to approach this. Diagnostics.
*** Evaluating a Hypothesis
[2016-08-30 Tue 12:03]
Small number of features can plot points/hypothesis, but doesn't scale well. Can split data into train and test sets (70/30 split). Compute test error J_{test}(\theta). Misclassification error (0/1 error). Look at output of 1 if misclassified, 0 if classified correctly.
*** Model Selection and Train/Validation/Test Sets
[2016-08-30 Tue 12:03]
Model selection - d is the degree of polynomial for model chosen. Can take all your models, compute \theta for each one, then compute test error. Problem is that this degree is not going to be a fair test, will be overly optimistic. Split data into test (60%), cross validation (20%) and training (20%). Now can compute three sets of errors, for test, cross-validation and training. Use training set to train your models, validation set to pick the model (degree of polynomial), and test to validate. Fit an extra parameter (d) to the CV data.
** Bias vs. Variance
[2016-08-30 Tue 12:03]
*** Diagnosing Bias vs. Variance
[2016-08-30 Tue 12:03]
High bias - underfit. High variance - overfit. Plot of error as a funciton of degree of polynomial. For training set, see a decrease as degree increases. CV error plots more like a parabola. If J_{train} and J_{cv} are high and fairly close, then you have high bias. If you have low J_{train} and J_{cv} much higher than J_{train}, then most likely overfit/high variance. 
*** Regularization and Bias/Variance
[2016-08-30 Tue 12:04]
Set J_{train}, J_{cv} and J_{test} to be just the average sum of squares of errors (without the regularization term). Range of \lambdas, minimize the cost functions for training set and evaluate J_{cv}(\theta) (without regulariztion) to pick a model. Then, with that model test using the test data set.

Need to look at this much more.
*** Learning Curves
[2016-08-30 Tue 12:04]
Inverted curves. For high bias, J_{cv} and J_{train} are fairly close together as # of samples increases. For high variance, J_{cv} and J_{train} are pretty far apart as m goes up. When you extrapolate out, more data will help close the gap for the high variance case.
*** Deciding What to Do Next Revisited
[2016-08-30 Tue 12:04]
Back to original question. More examples or smaller sets of features are solutions for high variance, additional features or adding polynomial features helps for high bias. Decreasing \lambda helps for high bias, increasing \lambda helps for high variance.

Neural networks - smaller networks computationally easier, but more prone to underfitting. Larger networks have more parameters and more prone to overfitting and are computationally more expensive, but can be helped with regularization (\lambda). 

How to prove the \lambda observations?
** Building a Spam Classifier
[2016-09-28 Wed 14:39]
*** Prioritizing What to Work On
[2016-09-28 Wed 14:41]
Ways to classify spam -- can choose a list of words likely to be spam. Classify as spam (y == 1) or not (y == 0). Feature vector (x) is either a 0 if the word in your dictionary appears, and 1 if it appears once or more. How to reduce error? Collect more data, look at more sophisticated features (e.g. mail routing headers), punctuation or misspellings (``med1cine''), other dictionary questions. Need to avoid fixating on any one option. 
*** Error Analysis
[2016-09-28 Wed 14:41]
Start simply with algo you can implement quickly. Plot learning curves to guide you in what direction to go next (bias vs variance, etc.). (Manually) look for trends in errors from the cross-validation set, to see if you can figure out systematic errors. In this case, categorize the types of e-mails that are misclassified, and see if there's additional cues/features that we could use to classify better.

Numerical evaluation (spit out a number on how well you're doing). In this case, ``stemming'' (Porter stemmer), should discount be the same as discounts? Cross-validation error.


** Handling Skewed Data 
[2016-09-28 Wed 14:40]
*** Error Metrics for Skewed Classes
[2016-09-28 Wed 14:40]
Skewed classes -- when you have a lot more examples of one class versus another (0.50% of all cases are y == 1). Becomes much harder to calculate error and if we're improving, in this case if we just set y = 0 we can do pretty good without any learning. True positive/true negative -- accurate prediction. Predict positive when we don't (false positive) and predict negative when actually positive (false negative). Precision -- ratio of true positives to true positives plus false positives. Recall -- ratio of true positives to true positives plus false negatives.
*** Trading off Precision and Recall
[2016-09-28 Wed 14:40]
Started with predicting if h_{0}(x) >= 0.5, and 0 if < 0.5, but can increase the thresholds to avoid false positives. This gives higher precision, but lower recall. On the other hand, if we want to avoid false negatives, then we lower the threshold to give higher recall at a cost of lower precision. Can plot this as a curve. How do you compare algorithms to pick? Average P + R / 2, but can be skewed by extremes (predict y == 1 all the time). Use the F score (F_{1} score). 2 * \frac{P * R}{P + R}. Gives you a value between 0 (worst case) and 1 (best case, perfect recall and perfect precision).
** Using Large Data Sets
[2016-09-28 Wed 14:40]
*** Data for Machine Learning
[2016-09-28 Wed 14:40]
Banko and Brill study in 2001. Found over four different ML algorithms, more data gives monotonically increasing precision. ``It's not who has the best algorithm that wins, it's who has the most data.''

Question -- if we go to a human with our input features (x), can a human expert predict y accurately?

In cases where we have lots of parameters, it helps to ensure low bias. By then having lots of data, this helps to keep the variance low as well.
** Quiz
[2016-08-30 Tue 12:03]
** Programming Assignment
[2016-08-30 Tue 12:03]
